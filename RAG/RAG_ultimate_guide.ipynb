{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello Kagglers! RAG, which stands for Retrieval Augmented Generation, is a technique used to enhance the knowledge base of large language models (LLMs) through the integration of external information. By doing so, LLMs are equipped to generate more context-aware responses and reduce instances of hallucination. This guide aims to offer an in-depth exploration of the RAG process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Let's look at some reasons why we may need RAG. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37832aea4ce74b8693b03f59fa7e9ba5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import GemmaTokenizer, GemmaForCausalLM\n",
    "import torch\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model= GemmaForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2b-it\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "\n",
    "tokenizer = GemmaTokenizer.from_pretrained(\"google/gemma-2b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the prompt \n",
    "prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(query, input=None):\n",
    "        \n",
    "    inputs = tokenizer(\n",
    "    [\n",
    "        prompt.format(\n",
    "            query, # instruction\n",
    "            \"\", # input\n",
    "            \"\", # output - leave this blank for generation!\n",
    "        )\n",
    "    ], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(**inputs,  max_new_tokens = 2048, use_cache = True)\n",
    "    answer = tokenizer.batch_decode(outputs)[0]\n",
    "    return answer.split(\"Response:\\n\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 'Attention is all you need' paper introduced the concept of self-attention, a mechanism that allows each element in the input sequence to attend to all other elements, regardless of their distance. This has significantly improved the performance of machine translation and other natural language processing tasks.<eos>\n"
     ]
    }
   ],
   "source": [
    "print(get_response(\"What are the contributions of the 'Attention is all you need' paper?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out the Gemma model really hit the mark with its answer this time. Chances are, it's familiar with the super popular paper [\"Attention is all you need,\"](https://arxiv.org/abs/1706.03762) which likely showed up in its training data. Now, let's switch gears and check out something newer on the scene: the paper titled [\"Genie: Generative Interactive Environments\"](https://arxiv.org/abs/2402.15391)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 'Genie: Generative Interactive Environments' paper focuses on the development of a novel generative framework called Genie that can create interactive environments that are both creative and engaging. The paper explores the following contributions of Genie:\n",
      "\n",
      "* **Generative capabilities:** Genie can generate diverse and realistic interactive environments, including physical, virtual, and mixed-reality environments.\n",
      "* **Interactive design:** Genie's interactive design process is based on the principles of human-centered design and focuses on creating environments that are intuitive and easy to use.\n",
      "* **Multi-modal integration:** Genie can integrate with various modalities, including visual, auditory, and haptic feedback, to create immersive and multi-sensory experiences.\n",
      "* **Adaptive behavior:** Genie can adapt its behavior to the individual user, providing personalized and interactive experiences.\n",
      "* **Social interaction:** Genie can facilitate social interaction through the integration of social agents and collaborative tools.\n",
      "* **Educational potential:** Genie can be used for educational purposes, as it can provide interactive and engaging learning experiences.\n",
      "\n",
      "Genie is a significant contribution to the field of interactive environments, as it presents a novel approach to creating interactive environments that are not only creative but also engaging and effective.<eos>\n"
     ]
    }
   ],
   "source": [
    "print(get_response(\"What are the contributions of the 'Genie: Generative Interactive Environments' paper?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woah! Looks like the model just hallucinate a bunch of false information instead of something helpful. This probably happened because Gemma isn't familiar with the \"Genie\" paper. It seems like this paper didn't make it into its pre-trained knowledge base.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what's the fix here? You might think about retraining Gemma from scratch with the latest info, but let's be real—that's a no-go. Training these massive language models like Gemma from the ground up costs a fortune in time and money! And that's exactly where RAG comes to the rescue. The beauty of RAG technology is that it saves us from having to retrain the whole massive model every single time we need it to learn something new. Instead, we can just hook up the relevant knowledge bases as extra input for the model, boosting the accuracy of its responses without breaking the bank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Retrieval Augmented Generation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG, short for Retrieval Augmented Generation, enhances the capabilities of large language models (LLMs) by incorporating a retrieval step into the process. When tasked with answering a question or generating text, RAG first seeks out relevant information from a vast repository of knowledge, which could include an array of documents and web pages. This approach allows the model to refine its generated responses by integrating this retrieved information, offering a more informed output that extends beyond its pre-trained knowledge base.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](./images/1_bo0JwTdru5quxDiPFa1TvA-ezgif.com-webp-to-png-converter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "picture coming from [this](https://ai.plainenglish.io/a-brief-introduction-to-retrieval-augmented-generation-rag-b7eb70982891) amazing blog post\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, there are 3 main steps in a RAG pipeline\n",
    "- Indexing: The indexing process involves cleaning raw data and converting it to plain text from formats like PDF and HTML. This text is then divided into smaller pieces and converted into vectors. Finally, an index stores these pieces and their vectors for efficient searching.\n",
    "\n",
    "- Retrieval: Retrieve relevant information from external sources based on user query. To find relevant information based on a user's query, the system performs a vector search or a hybrid search within a vector database. \n",
    "\n",
    "- Generation: When a user poses a query, RAG takes that along with the context it retrieved and feeds them both into the large language model (LLM). This process enables the LLM to produce a more informed and accurate response by considering both the user's original question and the additional information sourced from the knowledge database.\n",
    "\n",
    "Before diving into each part, let's define what we wanto to make!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, I want to design a chatbot that have the ability to understand and explain basic concepts about data science, machine learning, deep learning.\n",
    "\n",
    "But not only that I want it as my personal research assistant, with the ability to:\n",
    "- Find the latest papers, and give me a short overview of these papers. \n",
    "- Explore and list all the papers of a certain topic.\n",
    "- Could suggest some concepts that I could explore to understand a specific paper.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 1: Indexing - Creating Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start our exploration by zeroing in on the Indexing bit of RAG. Think of an Index as a cleverly organized digital filing cabinet, stuffed with Documents that a language model can sift through for answers. In this tutorial, we're going to use the `VectorStoreIndex` from llamaindex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.1 Getting data from Arxiv**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But first we need to prepare some data, in this note book, I will use the [arxiv dataset](https://www.kaggle.com/datasets/Cornell-University/arxiv/data)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Arxiv category codes\n",
    "# Source: https://www.kaggle.com/code/artgor/arxiv-metadata-exploration\n",
    "\n",
    "# https://arxiv.org/category_taxonomy\n",
    "# https://info.arxiv.org/help/api/user-manual.html#subject_classifications\n",
    "\n",
    "\n",
    "category_map = {\n",
    "# These created errors when mapping categories to descriptions\n",
    "'acc-phys': 'Accelerator Physics',\n",
    "'adap-org': 'Not available',\n",
    "'q-bio': 'Not available',\n",
    "'cond-mat': 'Not available',\n",
    "'chao-dyn': 'Not available',\n",
    "'patt-sol': 'Not available',\n",
    "'dg-ga': 'Not available',\n",
    "'solv-int': 'Not available',\n",
    "'bayes-an': 'Not available',\n",
    "'comp-gas': 'Not available',\n",
    "'alg-geom': 'Not available',\n",
    "'funct-an': 'Not available',\n",
    "'q-alg': 'Not available',\n",
    "'ao-sci': 'Not available',\n",
    "'atom-ph': 'Atomic Physics',\n",
    "'chem-ph': 'Chemical Physics',\n",
    "'plasm-ph': 'Plasma Physics',\n",
    "'mtrl-th': 'Not available',\n",
    "'cmp-lg': 'Not available',\n",
    "'supr-con': 'Not available',\n",
    "###\n",
    "\n",
    "# Added\n",
    "'econ.GN': 'General Economics', \n",
    "'econ.TH': 'Theoretical Economics', \n",
    "'eess.SY': 'Systems and Control', \n",
    "    \n",
    "'astro-ph': 'Astrophysics',\n",
    "'astro-ph.CO': 'Cosmology and Nongalactic Astrophysics',\n",
    "'astro-ph.EP': 'Earth and Planetary Astrophysics',\n",
    "'astro-ph.GA': 'Astrophysics of Galaxies',\n",
    "'astro-ph.HE': 'High Energy Astrophysical Phenomena',\n",
    "'astro-ph.IM': 'Instrumentation and Methods for Astrophysics',\n",
    "'astro-ph.SR': 'Solar and Stellar Astrophysics',\n",
    "'cond-mat.dis-nn': 'Disordered Systems and Neural Networks',\n",
    "'cond-mat.mes-hall': 'Mesoscale and Nanoscale Physics',\n",
    "'cond-mat.mtrl-sci': 'Materials Science',\n",
    "'cond-mat.other': 'Other Condensed Matter',\n",
    "'cond-mat.quant-gas': 'Quantum Gases',\n",
    "'cond-mat.soft': 'Soft Condensed Matter',\n",
    "'cond-mat.stat-mech': 'Statistical Mechanics',\n",
    "'cond-mat.str-el': 'Strongly Correlated Electrons',\n",
    "'cond-mat.supr-con': 'Superconductivity',\n",
    "'cs.AI': 'Artificial Intelligence',\n",
    "'cs.AR': 'Hardware Architecture',\n",
    "'cs.CC': 'Computational Complexity',\n",
    "'cs.CE': 'Computational Engineering, Finance, and Science',\n",
    "'cs.CG': 'Computational Geometry',\n",
    "'cs.CL': 'Computation and Language',\n",
    "'cs.CR': 'Cryptography and Security',\n",
    "'cs.CV': 'Computer Vision and Pattern Recognition',\n",
    "'cs.CY': 'Computers and Society',\n",
    "'cs.DB': 'Databases',\n",
    "'cs.DC': 'Distributed, Parallel, and Cluster Computing',\n",
    "'cs.DL': 'Digital Libraries',\n",
    "'cs.DM': 'Discrete Mathematics',\n",
    "'cs.DS': 'Data Structures and Algorithms',\n",
    "'cs.ET': 'Emerging Technologies',\n",
    "'cs.FL': 'Formal Languages and Automata Theory',\n",
    "'cs.GL': 'General Literature',\n",
    "'cs.GR': 'Graphics',\n",
    "'cs.GT': 'Computer Science and Game Theory',\n",
    "'cs.HC': 'Human-Computer Interaction',\n",
    "'cs.IR': 'Information Retrieval',\n",
    "'cs.IT': 'Information Theory',\n",
    "'cs.LG': 'Machine Learning',\n",
    "'cs.LO': 'Logic in Computer Science',\n",
    "'cs.MA': 'Multiagent Systems',\n",
    "'cs.MM': 'Multimedia',\n",
    "'cs.MS': 'Mathematical Software',\n",
    "'cs.NA': 'Numerical Analysis',\n",
    "'cs.NE': 'Neural and Evolutionary Computing',\n",
    "'cs.NI': 'Networking and Internet Architecture',\n",
    "'cs.OH': 'Other Computer Science',\n",
    "'cs.OS': 'Operating Systems',\n",
    "'cs.PF': 'Performance',\n",
    "'cs.PL': 'Programming Languages',\n",
    "'cs.RO': 'Robotics',\n",
    "'cs.SC': 'Symbolic Computation',\n",
    "'cs.SD': 'Sound',\n",
    "'cs.SE': 'Software Engineering',\n",
    "'cs.SI': 'Social and Information Networks',\n",
    "'cs.SY': 'Systems and Control',\n",
    "'econ.EM': 'Econometrics',             \n",
    "'eess.AS': 'Audio and Speech Processing',\n",
    "'eess.IV': 'Image and Video Processing',\n",
    "'eess.SP': 'Signal Processing',               \n",
    "'gr-qc': 'General Relativity and Quantum Cosmology',\n",
    "'hep-ex': 'High Energy Physics - Experiment',\n",
    "'hep-lat': 'High Energy Physics - Lattice',\n",
    "'hep-ph': 'High Energy Physics - Phenomenology',\n",
    "'hep-th': 'High Energy Physics - Theory',\n",
    "'math.AC': 'Commutative Algebra',\n",
    "'math.AG': 'Algebraic Geometry',\n",
    "'math.AP': 'Analysis of PDEs',\n",
    "'math.AT': 'Algebraic Topology',\n",
    "'math.CA': 'Classical Analysis and ODEs',\n",
    "'math.CO': 'Combinatorics',\n",
    "'math.CT': 'Category Theory',\n",
    "'math.CV': 'Complex Variables',\n",
    "'math.DG': 'Differential Geometry',\n",
    "'math.DS': 'Dynamical Systems',\n",
    "'math.FA': 'Functional Analysis',\n",
    "'math.GM': 'General Mathematics',\n",
    "'math.GN': 'General Topology',\n",
    "'math.GR': 'Group Theory',\n",
    "'math.GT': 'Geometric Topology',\n",
    "'math.HO': 'History and Overview',\n",
    "'math.IT': 'Information Theory',\n",
    "'math.KT': 'K-Theory and Homology',\n",
    "'math.LO': 'Logic',\n",
    "'math.MG': 'Metric Geometry',\n",
    "'math.MP': 'Mathematical Physics',\n",
    "'math.NA': 'Numerical Analysis',\n",
    "'math.NT': 'Number Theory',\n",
    "'math.OA': 'Operator Algebras',\n",
    "'math.OC': 'Optimization and Control',\n",
    "'math.PR': 'Probability',\n",
    "'math.QA': 'Quantum Algebra',\n",
    "'math.RA': 'Rings and Algebras',\n",
    "'math.RT': 'Representation Theory',\n",
    "'math.SG': 'Symplectic Geometry',\n",
    "'math.SP': 'Spectral Theory',\n",
    "'math.ST': 'Statistics Theory',\n",
    "'math-ph': 'Mathematical Physics',\n",
    "'nlin.AO': 'Adaptation and Self-Organizing Systems',\n",
    "'nlin.CD': 'Chaotic Dynamics',\n",
    "'nlin.CG': 'Cellular Automata and Lattice Gases',\n",
    "'nlin.PS': 'Pattern Formation and Solitons',\n",
    "'nlin.SI': 'Exactly Solvable and Integrable Systems',\n",
    "'nucl-ex': 'Nuclear Experiment',\n",
    "'nucl-th': 'Nuclear Theory',\n",
    "'physics.acc-ph': 'Accelerator Physics',\n",
    "'physics.ao-ph': 'Atmospheric and Oceanic Physics',\n",
    "'physics.app-ph': 'Applied Physics',\n",
    "'physics.atm-clus': 'Atomic and Molecular Clusters',\n",
    "'physics.atom-ph': 'Atomic Physics',\n",
    "'physics.bio-ph': 'Biological Physics',\n",
    "'physics.chem-ph': 'Chemical Physics',\n",
    "'physics.class-ph': 'Classical Physics',\n",
    "'physics.comp-ph': 'Computational Physics',\n",
    "'physics.data-an': 'Data Analysis, Statistics and Probability',\n",
    "'physics.ed-ph': 'Physics Education',\n",
    "'physics.flu-dyn': 'Fluid Dynamics',\n",
    "'physics.gen-ph': 'General Physics',\n",
    "'physics.geo-ph': 'Geophysics',\n",
    "'physics.hist-ph': 'History and Philosophy of Physics',\n",
    "'physics.ins-det': 'Instrumentation and Detectors',\n",
    "'physics.med-ph': 'Medical Physics',\n",
    "'physics.optics': 'Optics',\n",
    "'physics.plasm-ph': 'Plasma Physics',\n",
    "'physics.pop-ph': 'Popular Physics',\n",
    "'physics.soc-ph': 'Physics and Society',\n",
    "'physics.space-ph': 'Space Physics',\n",
    "'q-bio.BM': 'Biomolecules',\n",
    "'q-bio.CB': 'Cell Behavior',\n",
    "'q-bio.GN': 'Genomics',\n",
    "'q-bio.MN': 'Molecular Networks',\n",
    "'q-bio.NC': 'Neurons and Cognition',\n",
    "'q-bio.OT': 'Other Quantitative Biology',\n",
    "'q-bio.PE': 'Populations and Evolution',\n",
    "'q-bio.QM': 'Quantitative Methods',\n",
    "'q-bio.SC': 'Subcellular Processes',\n",
    "'q-bio.TO': 'Tissues and Organs',\n",
    "'q-fin.CP': 'Computational Finance',\n",
    "'q-fin.EC': 'Economics',\n",
    "'q-fin.GN': 'General Finance',\n",
    "'q-fin.MF': 'Mathematical Finance',\n",
    "'q-fin.PM': 'Portfolio Management',\n",
    "'q-fin.PR': 'Pricing of Securities',\n",
    "'q-fin.RM': 'Risk Management',\n",
    "'q-fin.ST': 'Statistical Finance',\n",
    "'q-fin.TR': 'Trading and Market Microstructure',\n",
    "'quant-ph': 'Quantum Physics',\n",
    "'stat.AP': 'Applications',\n",
    "'stat.CO': 'Computation',\n",
    "'stat.ME': 'Methodology',\n",
    "'stat.ML': 'Machine Learning',\n",
    "'stat.OT': 'Other Statistics',\n",
    "'stat.TH': 'Statistics Theory'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2436004, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0704.0001</td>\n",
       "      <td>Calculation of prompt diphoton production cros...</td>\n",
       "      <td>A fully differential calculation in perturba...</td>\n",
       "      <td>hep-ph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0704.0002</td>\n",
       "      <td>Sparsity-certifying Graph Decompositions</td>\n",
       "      <td>We describe a new algorithm, the $(k,\\ell)$-...</td>\n",
       "      <td>math.CO cs.CG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0704.0003</td>\n",
       "      <td>The evolution of the Earth-Moon system based o...</td>\n",
       "      <td>The evolution of Earth-Moon system is descri...</td>\n",
       "      <td>physics.gen-ph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0704.0004</td>\n",
       "      <td>A determinant of Stirling cycle numbers counts...</td>\n",
       "      <td>We show that a determinant of Stirling cycle...</td>\n",
       "      <td>math.CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0704.0005</td>\n",
       "      <td>From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...</td>\n",
       "      <td>In this paper we show how to compute the $\\L...</td>\n",
       "      <td>math.CA math.FA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              title  \\\n",
       "0  0704.0001  Calculation of prompt diphoton production cros...   \n",
       "1  0704.0002           Sparsity-certifying Graph Decompositions   \n",
       "2  0704.0003  The evolution of the Earth-Moon system based o...   \n",
       "3  0704.0004  A determinant of Stirling cycle numbers counts...   \n",
       "4  0704.0005  From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...   \n",
       "\n",
       "                                            abstract       categories  \n",
       "0    A fully differential calculation in perturba...           hep-ph  \n",
       "1    We describe a new algorithm, the $(k,\\ell)$-...    math.CO cs.CG  \n",
       "2    The evolution of Earth-Moon system is descri...   physics.gen-ph  \n",
       "3    We show that a determinant of Stirling cycle...          math.CO  \n",
       "4    In this paper we show how to compute the $\\L...  math.CA math.FA  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.kaggle.com/code/matthewmaddock/nlp-arxiv-dataset-transformers-and-umap\n",
    "\n",
    "# This takes about 1 minute.\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "cols = ['id', 'title', 'abstract', 'categories']\n",
    "data = []\n",
    "file_name = './input/arxiv-metadata-oai-snapshot.json'\n",
    "\n",
    "\n",
    "with open(file_name, encoding='latin-1') as f:\n",
    "    for line in f:\n",
    "        doc = json.loads(line)\n",
    "        lst = [doc['id'], doc['title'], doc['abstract'], doc['categories']]\n",
    "        data.append(lst)\n",
    "\n",
    "df_data = pd.DataFrame(data=data, columns=cols)\n",
    "\n",
    "print(df_data.shape)\n",
    "\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's filter out topics that are not about data science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ['cs.AI', 'cs.CV', 'cs.IR', 'cs.LG', 'cs.CL']\n",
    "\n",
    "filtered_data = df_data[df_data['categories'].isin(topics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106589"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = filtered_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>categories</th>\n",
       "      <th>cat_text</th>\n",
       "      <th>prepared_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1266</th>\n",
       "      <td>0704.1267</td>\n",
       "      <td>Text Line Segmentation of Historical Documents...</td>\n",
       "      <td>There is a huge amount of historical documents...</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>Computer Vision and Pattern Recognition</td>\n",
       "      <td>Text Line Segmentation of Historical Documents...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1273</th>\n",
       "      <td>0704.1274</td>\n",
       "      <td>Parametric Learning and Monte Carlo Optimization</td>\n",
       "      <td>This paper uncovers and explores the close rel...</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>Machine Learning</td>\n",
       "      <td>Parametric Learning and Monte Carlo Optimizati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1393</th>\n",
       "      <td>0704.1394</td>\n",
       "      <td>Calculating Valid Domains for BDD-Based Intera...</td>\n",
       "      <td>In these notes we formally describe the functi...</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>Calculating Valid Domains for BDD-Based Intera...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>0704.2010</td>\n",
       "      <td>A study of structural properties on profiles HMMs</td>\n",
       "      <td>Motivation: Profile hidden Markov Models (pHMM...</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>A study of structural properties on profiles H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2667</th>\n",
       "      <td>0704.2668</td>\n",
       "      <td>Supervised Feature Selection via Dependence Es...</td>\n",
       "      <td>We introduce a framework for filtering feature...</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>Machine Learning</td>\n",
       "      <td>Supervised Feature Selection via Dependence Es...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                              title  \\\n",
       "1266  0704.1267  Text Line Segmentation of Historical Documents...   \n",
       "1273  0704.1274   Parametric Learning and Monte Carlo Optimization   \n",
       "1393  0704.1394  Calculating Valid Domains for BDD-Based Intera...   \n",
       "2009  0704.2010  A study of structural properties on profiles HMMs   \n",
       "2667  0704.2668  Supervised Feature Selection via Dependence Es...   \n",
       "\n",
       "                                               abstract categories  \\\n",
       "1266  There is a huge amount of historical documents...      cs.CV   \n",
       "1273  This paper uncovers and explores the close rel...      cs.LG   \n",
       "1393  In these notes we formally describe the functi...      cs.AI   \n",
       "2009  Motivation: Profile hidden Markov Models (pHMM...      cs.AI   \n",
       "2667  We introduce a framework for filtering feature...      cs.LG   \n",
       "\n",
       "                                     cat_text  \\\n",
       "1266  Computer Vision and Pattern Recognition   \n",
       "1273                         Machine Learning   \n",
       "1393                  Artificial Intelligence   \n",
       "2009                  Artificial Intelligence   \n",
       "2667                         Machine Learning   \n",
       "\n",
       "                                          prepared_text  \n",
       "1266  Text Line Segmentation of Historical Documents...  \n",
       "1273  Parametric Learning and Monte Carlo Optimizati...  \n",
       "1393  Calculating Valid Domains for BDD-Based Intera...  \n",
       "2009  A study of structural properties on profiles H...  \n",
       "2667  Supervised Feature Selection via Dependence Es...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.kaggle.com/code/vbookshelf/part-1-build-an-arxiv-rag-search-system-w-faiss\n",
    "\n",
    "def get_cat_text(x):\n",
    "    \n",
    "    cat_text = ''\n",
    "    \n",
    "    # Put the codes into a list\n",
    "    cat_list = x.split(' ')\n",
    "    \n",
    "    for i, item in enumerate(cat_list):\n",
    "        \n",
    "        cat_name = category_map[item]\n",
    "        \n",
    "        # If there was no description available\n",
    "        # for the category code then don't include it in the text.\n",
    "        if cat_name != 'Not available':\n",
    "            \n",
    "            if i == 0:\n",
    "                cat_text = cat_name\n",
    "            else:\n",
    "                cat_text = cat_text + ', ' + cat_name\n",
    " \n",
    "    # Remove leading and trailing spaces\n",
    "    cat_text = cat_text.strip()\n",
    "    \n",
    "    return cat_text\n",
    "    \n",
    "\n",
    "df_data['cat_text'] = df_data['categories'].apply(get_cat_text)\n",
    "\n",
    "def clean_text(x):\n",
    "    \n",
    "    # Replace newline characters with a space\n",
    "    new_text = x.replace(\"\\n\", \" \")\n",
    "    # Remove leading and trailing spaces\n",
    "    new_text = new_text.strip()\n",
    "    \n",
    "    return new_text\n",
    "\n",
    "df_data['title'] = df_data['title'].apply(clean_text)\n",
    "df_data['abstract'] = df_data['abstract'].apply(clean_text)\n",
    "\n",
    "df_data['prepared_text'] = df_data['title'] + ' \\n ' + df_data['abstract']\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "arxiv_documents = [Document(text=item) for item in list(df_data['prepared_text'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_df = df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.2 Getting data from wikipedia**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U wikipedia-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Pre-compile the regular expression pattern for better performance\n",
    "BRACES_PATTERN = re.compile(r'\\{.*?\\}|\\}')\n",
    "\n",
    "def remove_braces_and_content(text):\n",
    "    \"\"\"Remove all occurrences of curly braces and their content from the given text\"\"\"\n",
    "    return BRACES_PATTERN.sub('', text)\n",
    "\n",
    "def clean_string(input_string):\n",
    "    \"\"\"Clean the input string.\"\"\"\n",
    "    \n",
    "    # Remove extra spaces by splitting the string by spaces and joining back together\n",
    "    cleaned_string = ' '.join(input_string.split())\n",
    "    \n",
    "    # Remove consecutive carriage return characters until there are no more consecutive occurrences\n",
    "    cleaned_string = re.sub(r'\\r+', '\\r', cleaned_string)\n",
    "    \n",
    "    # Remove all occurrences of curly braces and their content from the cleaned string\n",
    "    cleaned_string = remove_braces_and_content(cleaned_string)\n",
    "    \n",
    "    # Return the cleaned string\n",
    "    return cleaned_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_wikipedia_pages(wiki_wiki, category_name):\n",
    "    \"\"\"Extract all references from a category on Wikipedia\"\"\"\n",
    "    \n",
    "    # Get the Wikipedia page corresponding to the provided category name\n",
    "    category = wiki_wiki.page(\"Category:\" + category_name)\n",
    "    \n",
    "    # Initialize an empty list to store page titles\n",
    "    pages = []\n",
    "    \n",
    "    # Check if the category exists\n",
    "    if category.exists():\n",
    "        # Iterate through each article in the category and append its title to the list\n",
    "        for article in category.categorymembers.values():\n",
    "            pages.append(article.title)\n",
    "    \n",
    "    # Return the list of page titles\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_wikipedia_pages(categories):\n",
    "    \"\"\"Retrieve Wikipedia pages from a list of categories and extract their content\"\"\"\n",
    "    \n",
    "    # Create a Wikipedia object\n",
    "    wiki_wiki = wikipediaapi.Wikipedia('Kaggle Data Science Assistant with Gemma', 'en')\n",
    "    \n",
    "    # Initialize lists to store explored categories and Wikipedia pages\n",
    "    explored_categories = []\n",
    "    wikipedia_pages = []\n",
    "\n",
    "    # Iterate through each category\n",
    "    print(\"- Processing Wikipedia categories:\")\n",
    "    for category_name in categories:\n",
    "        print(f\"\\tExploring {category_name} on Wikipedia\")\n",
    "        \n",
    "        # Get the Wikipedia page corresponding to the category\n",
    "        category = wiki_wiki.page(\"Category:\" + category_name)\n",
    "        \n",
    "        # Extract Wikipedia pages from the category and extend the list\n",
    "        wikipedia_pages.extend(extract_wikipedia_pages(wiki_wiki, category_name))\n",
    "        \n",
    "        # Add the explored category to the list\n",
    "        explored_categories.append(category_name)\n",
    "\n",
    "    # Extract subcategories and remove duplicate categories\n",
    "    categories_to_explore = [item.replace(\"Category:\", \"\") for item in wikipedia_pages if \"Category:\" in item]\n",
    "    wikipedia_pages = list(set([item for item in wikipedia_pages if \"Category:\" not in item]))\n",
    "    \n",
    "    # Explore subcategories recursively\n",
    "    while categories_to_explore:\n",
    "        category_name = categories_to_explore.pop()\n",
    "        print(f\"\\tExploring {category_name} on Wikipedia\")\n",
    "        \n",
    "        # Extract more references from the subcategory\n",
    "        more_refs = extract_wikipedia_pages(wiki_wiki, category_name)\n",
    "\n",
    "        # Iterate through the references\n",
    "        for ref in more_refs:\n",
    "            # Check if the reference is a category\n",
    "            if \"Category:\" in ref:\n",
    "                new_category = ref.replace(\"Category:\", \"\")\n",
    "                # Add the new category to the explored categories list\n",
    "                if new_category not in explored_categories:\n",
    "                    explored_categories.append(new_category)\n",
    "            else:\n",
    "                # Add the reference to the Wikipedia pages list\n",
    "                if ref not in wikipedia_pages:\n",
    "                    wikipedia_pages.append(ref)\n",
    "\n",
    "    # Initialize a list to store extracted texts\n",
    "    extracted_texts = []\n",
    "    \n",
    "    # Iterate through each Wikipedia page\n",
    "    print(\"- Processing Wikipedia pages:\")\n",
    "    for page_title in tqdm(wikipedia_pages):\n",
    "        # Get the Wikipedia page\n",
    "        page = wiki_wiki.page(page_title)\n",
    "        \n",
    "        # Check if the page summary does not contain certain keywords\n",
    "        if \"Biden\" not in page.summary and \"Trump\" not in page.summary:\n",
    "            # Append the page title and summary to the extracted texts list\n",
    "            if len(page.summary) > len(page.title):\n",
    "                extracted_texts.append(page.title + \" : \" + clean_string(page.summary))\n",
    "            \n",
    "            # Iterate through the sections in the page\n",
    "            for section in page.sections:\n",
    "                # Append the page title and section text to the extracted texts list\n",
    "                if len(section.text) > len(page.title):\n",
    "                    extracted_texts.append(page.title + \" : \" + clean_string(section.text))\n",
    "                    \n",
    "    # Return the extracted texts\n",
    "    return extracted_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Processing Wikipedia categories:\n",
      "\tExploring Machine_learning on Wikipedia\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'extract_wikipedia_pages' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m categories \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMachine_learning\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData_science\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStatistics\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeep_learning\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArtificial_intelligence\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m extracted_texts \u001b[38;5;241m=\u001b[39m \u001b[43mget_wikipedia_pages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcategories\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(extracted_texts), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWikipedia pages\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 23\u001b[0m, in \u001b[0;36mget_wikipedia_pages\u001b[0;34m(categories)\u001b[0m\n\u001b[1;32m     20\u001b[0m category \u001b[38;5;241m=\u001b[39m wiki_wiki\u001b[38;5;241m.\u001b[39mpage(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCategory:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m category_name)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Extract Wikipedia pages from the category and extend the list\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m wikipedia_pages\u001b[38;5;241m.\u001b[39mextend(\u001b[43mextract_wikipedia_pages\u001b[49m(wiki_wiki, category_name))\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Add the explored category to the list\u001b[39;00m\n\u001b[1;32m     26\u001b[0m explored_categories\u001b[38;5;241m.\u001b[39mappend(category_name)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'extract_wikipedia_pages' is not defined"
     ]
    }
   ],
   "source": [
    "categories = [\"Machine_learning\", \"Data_science\", \"Statistics\", \"Deep_learning\", \"Artificial_intelligence\"]\n",
    "extracted_texts = get_wikipedia_pages(categories)\n",
    "print(\"Found\", len(extracted_texts), \"Wikipedia pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'extracted_texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m wiki_documents \u001b[38;5;241m=\u001b[39m [Document(text\u001b[38;5;241m=\u001b[39mtext) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m \u001b[43mextracted_texts\u001b[49m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'extracted_texts' is not defined"
     ]
    }
   ],
   "source": [
    "wiki_documents = [Document(text=text) for text in extracted_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the length of the extracted text to get the optimal chunk size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'extracted_texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m wiki_lengths \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m \u001b[43mextracted_texts\u001b[49m])\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean Length:\u001b[39m\u001b[38;5;124m\"\u001b[39m,  wiki_lengths\u001b[38;5;241m.\u001b[39mmean())\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMax Length:\u001b[39m\u001b[38;5;124m\"\u001b[39m, wiki_lengths\u001b[38;5;241m.\u001b[39mmax())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'extracted_texts' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "wiki_lengths = np.array([len(text) for text in extracted_texts])\n",
    "print(\"Mean Length:\",  wiki_lengths.mean())\n",
    "print(\"Max Length:\", wiki_lengths.max())\n",
    "print(\"Min Length:\", wiki_lengths.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for document in arxiv_documents:\n",
    "    print(document.get_content())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_length = np.array([len(document.get_content()) for document in arxiv_documents])\n",
    "print(\"Mean Length:\",  arxiv_length.mean())\n",
    "print(\"Max Length:\", arxiv_length.max())\n",
    "print(\"Min Length:\", arxiv_length.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.3 Creating Index with `VectorStoreIndex`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `VectorStoreIndex` is by far the most frequently used type of Index in llamaindex. This class takes your Documents and splits them up into Nodes. Then, it creates `vector_embeddings` of the text of every node. But what is `vector_embedding`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector embeddings are like turning the essence of your words into a mathematical sketch. Imagine every idea or concept in your text getting its unique numerical fingerprint. This is handy because even if two snippets of text use different words, if they're sharing the same idea, their numerical sketches—or embeddings—will be close neighbors in the numerical space. This magic is done using tools known as embedding models.\n",
    "\n",
    "Choosing the right embedding model is crucial. It's like picking the right artist to paint your portrait; you want the one who captures you best. A great place to start is the MTEB leaderboard, where the crème de la crème of embedding models are ranked. As we have quite a large dataset, the model size matters, we don't want to wait all day for the model to extract all the vector embeddings. When I last checked, the `BAAI/bge-small-en-v1.5` model was leading the pack, especially considering its size. It could be a solid choice if you're diving into the world of text embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.embeddings.instructor import InstructorEmbedding\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "import chromadb\n",
    "import torch\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "\n",
    "# Create embed model\n",
    "device_type = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\", cache_folder=\"./models\", device=device_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we have to find somewhere to store all of the embeddings extracted by the model, and that's why we need a `vector store`. There are many to choose from, in this tutorial, I will choose the `chroma` vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.PersistentClient(path=\"./DB\")\n",
    "chroma_collection = chroma_client.get_or_create_collection(\"demo_arxiv\")\n",
    "\n",
    "\n",
    "# Create vector store\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 106589/106589 [00:26<00:00, 4022.57it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:04<00:00, 431.17it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:04<00:00, 480.15it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:04<00:00, 431.71it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:04<00:00, 427.49it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:04<00:00, 428.92it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:05<00:00, 406.85it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:05<00:00, 395.84it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:05<00:00, 396.65it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:05<00:00, 387.19it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:05<00:00, 382.53it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:05<00:00, 386.81it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:05<00:00, 380.95it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:05<00:00, 378.50it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:05<00:00, 363.96it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:05<00:00, 366.86it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:05<00:00, 372.67it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:05<00:00, 388.35it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:05<00:00, 376.82it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:05<00:00, 376.53it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:05<00:00, 372.63it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:05<00:00, 360.46it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:05<00:00, 376.34it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:05<00:00, 367.44it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:05<00:00, 364.16it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:05<00:00, 370.04it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:05<00:00, 373.11it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:05<00:00, 363.35it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:05<00:00, 363.37it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:05<00:00, 358.91it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:05<00:00, 353.91it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:05<00:00, 360.50it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:05<00:00, 365.17it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:05<00:00, 367.98it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:05<00:00, 363.78it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:05<00:00, 353.17it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:05<00:00, 346.49it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:05<00:00, 349.77it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:05<00:00, 360.93it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:05<00:00, 352.19it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:06<00:00, 339.07it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:05<00:00, 345.65it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:05<00:00, 351.98it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:05<00:00, 348.81it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:05<00:00, 347.41it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:05<00:00, 345.44it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:06<00:00, 341.17it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:06<00:00, 336.88it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:05<00:00, 347.72it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:05<00:00, 352.82it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:05<00:00, 350.39it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:05<00:00, 358.45it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:05<00:00, 389.73it/s]\n",
      "Generating embeddings: 100%|██████████| 93/93 [00:00<00:00, 487.02it/s]\n"
     ]
    }
   ],
   "source": [
    "index = VectorStoreIndex.from_documents(\n",
    "    arxiv_documents, storage_context=storage_context, embed_model=embed_model, show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(\n",
    "    wiki_documents, storage_context=storage_context, embed_model=embed_model, show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastic! We've successfully created a vector store for our data, laying down a solid foundation. To enhance this stage further, we could explore additional techniques like data preprocessing, text chunking, and node parsing. These methods can refine our data's quality and structure, potentially boosting our system's performance. However, to keep things straightforward and focused, we'll save these advanced topics for another time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.4 Loading from vector store**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you're executing this from a different script; there's no need to go through the hassle of recalculating the embeddings for all the documents again. You can simply load them up and dive straight into the task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "import torch\n",
    "\n",
    "device_type = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\", cache_folder=\"./models\", device=device_type) # must be the same as the previous stage\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=\"./DB\")\n",
    "chroma_collection = chroma_client.get_or_create_collection(\"demo_arxiv\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "# load the vectorstore\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_vector_store(vector_store, storage_context=storage_context, embed_model=embed_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's time to pivot to the next crucial phase: Retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 2: Retrieval**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.1 Basic Retrieval**\n",
    "\n",
    "In the realm of digital information retrieval, the process known as similarity search within vector databases stands out for its efficiency and precision. This begins when a user's query is transformed into a vector embedding by the `embedding model`, which must be consistent with the model used during the indexing phase to ensure compatibility. Subsequently, `VectorStoreIndex` executes a mathematical operation to arrange the embeddings according to their semantic similarity to the query. The number of embeddings returned, determined by the parameter 'k', defines the scope of the search results, commonly referred to as 'top_k'. This methodology, known as \"top-k semantic retrieval,\" is instrumental in refining search outcomes to present the most relevant results in a structured manner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_retriever(\n",
    "    similarity_top_k = 5, \n",
    "    alpha=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear discriminant analysis : Discriminant function analysis is very similar to logistic regression, and both can be used to answer the same research questions. Logistic regression does not have as many assumptions and restrictions as discriminant analysis. However, when discriminant analysis’ assumptions are met, it is more powerful than logistic regression. Unlike logistic regression, discriminant analysis can be used with small sample sizes. It has been shown that when sample sizes are equal, and homogeneity of variance/covariance holds, discriminant analysis is more accurate. Despite all these advantages, logistic regression has none-the-less become the common choice, since the assumptions of discriminant analysis are rarely met.\n",
      "=============\n",
      "An Experiment on Feature Selection using Logistic Regression \n",
      " In supervised machine learning, feature selection plays a very important role by potentially enhancing explainability and performance as measured by computing time and accuracy-related metrics. In this paper, we investigate a method for feature selection based on the well-known L1 and L2 regularization strategies associated with logistic regression (LR). It is well known that the learned coefficients, which serve as weights, can be used to rank the features. Our approach is to synthesize the findings of L1 and L2 regularization. For our experiment, we chose the CIC-IDS2018 dataset owing partly to its size and also to the existence of two problematic classes that are hard to separate. We report first with the exclusion of one of them and then with its inclusion. We ranked features first with L1 and then with L2, and then compared logistic regression with L1 (LR+L1) against that with L2 (LR+L2) by varying the sizes of the feature sets for each of the two rankings. We found no significant difference in accuracy between the two methods once the feature set is selected. We chose a synthesis, i.e., only those features that were present in both the sets obtained from L1 and that from L2, and experimented with it on more complex models like Decision Tree and Random Forest and observed that the accuracy was very close in spite of the small size of the feature set. Additionally, we also report on the standard metrics: accuracy, precision, recall, and f1-score.\n",
      "=============\n",
      "Log-linear model : A log-linear model is a mathematical model that takes the form of a function whose logarithm equals a linear combination of the parameters of the model, which makes it possible to apply (possibly multivariate) linear regression. That is, it has the general form exp ⁡ ( c + ∑ i w i f i ( X ) ) w_f_(X)\\right) ,in which the fi(X) are quantities that are functions of the variable X, in general a vector of values, while c and the wi stand for the model parameters. The term may specifically be used for: A log-linear plot or graph, which is a type of semi-log plot. Poisson regression for contingency tables, a type of generalized linear model.The specific applications of log-linear models are where the output quantity lies in the range 0 to ∞, for values of the independent variables X, or more immediately, the transformed quantities fi(X) in the range −∞ to +∞. This may be contrasted to logistic models, similar to the logistic function, for which the output quantity lies in the range 0 to 1. Thus the contexts where these models are useful or realistic often depends on the range of the values being modelled.\n",
      "=============\n",
      "Outline of regression analysis : Regression analysis Linear regression\n",
      "=============\n"
     ]
    }
   ],
   "source": [
    "for res in query_engine.retrieve(\"How is linear regression different from logistic regression?\"):\n",
    "    print(res.text)\n",
    "    print(\"=============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(chroma_collection.get()['documents'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2 Reranking**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a retriever pulls information from the vector store, it's a bit like casting a wide net – you end up with a lot of catches, but not all of them are the fish you're after. Some pieces of context can be way off the mark, leading us down the wrong path. That's where reranking comes into play. Think of reranking as a second round of scrutiny, a fine-tuning of sorts. After the initial haul from the vector search, reranking steps in to sift through the catch, reorganizing the order or ranking of the items (in this case, the documents we've retrieved) based on more specific criteria. It's like making sure the best, most relevant pieces of information are right at the top, ready for us to use. This extra step helps ensure that what we're working with is as relevant and useful as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how are rerankers different from our initial retriever?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Bi-Encoder vs Cross-Encoder.png](./images/Bi-Encoder%20vs%20Cross-Encoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conventional embedding model adheres to the Bi-Encoder paradigm, wherein embeddings for source documents are precomputed. During the query phase, the model generates an embedding for the user's query and then calculates the Cosine Similarity score across our database to identify the most relevant documents.\n",
    "\n",
    "For the reranking process, it is essential to input both the source documents and the query concurrently into the model. This allows the model to evaluate the similarity between the two entities. This approach can be considerably time-intensive, as it lacks the advantage of precomputed data. However, the potential for enhanced accuracy is substantial. Therefore, the reranking process is reserved for the top documents initially retrieved by the Bi-Encoder, ensuring a balance between efficiency and precision in the document selection process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reranking Cheatsheet**: Here is a useful reranking cheatsheet, originally in this [tweet](https://twitter.com/bclavie/status/1765312881120153659/photo/1). Thanks [@bclavie](https://twitter.com/bclavie)\n",
    "\n",
    "![GH-ms_HWcAEYWou.jpg](./images/GH-ms_HWcAEYWou.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "\n",
    "rerank_postprocessor = SentenceTransformerRerank(\n",
    "    model='mixedbread-ai/mxbai-rerank-xsmall-v1',\n",
    "    top_n=2, # number of nodes after re-ranking, \n",
    "    keep_retrieval_score=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "# re-define our query engine\n",
    "Settings.llm = None # We will touch this in the next section\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=10,  # Number of nodes before re-ranking\n",
    "    node_postprocessors=[rerank_postprocessor],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is below.\n",
      "---------------------\n",
      "retrieval_score: 0.4866113187679043\n",
      "\n",
      "Tune-A-Video: One-Shot Tuning of Image Diffusion Models for   Text-to-Video Generation \n",
      " To replicate the success of text-to-image (T2I) generation, recent works employ large-scale video datasets to train a text-to-video (T2V) generator. Despite their promising results, such paradigm is computationally expensive. In this work, we propose a new T2V generation setting$\\unicode{x2014}$One-Shot Video Tuning, where only one text-video pair is presented. Our model is built on state-of-the-art T2I diffusion models pre-trained on massive image data. We make two key observations: 1) T2I models can generate still images that represent verb terms; 2) extending T2I models to generate multiple images concurrently exhibits surprisingly good content consistency. To further learn continuous motion, we introduce Tune-A-Video, which involves a tailored spatio-temporal attention mechanism and an efficient one-shot tuning strategy. At inference, we employ DDIM inversion to provide structure guidance for sampling. Extensive qualitative and numerical experiments demonstrate the remarkable ability of our method across various applications.\n",
      "\n",
      "retrieval_score: 0.441578197230505\n",
      "\n",
      "DGL: Dynamic Global-Local Prompt Tuning for Text-Video Retrieval \n",
      " Text-video retrieval is a critical multi-modal task to find the most relevant video for a text query. Although pretrained models like CLIP have demonstrated impressive potential in this area, the rising cost of fully finetuning these models due to increasing model size continues to pose a problem. To address this challenge, prompt tuning has emerged as an alternative. However, existing works still face two problems when adapting pretrained image-text models to downstream video-text tasks: (1) The visual encoder could only encode frame-level features and failed to extract global-level general video information. (2) Equipping the visual and text encoder with separated prompts failed to mitigate the visual-text modality gap. To this end, we propose DGL, a cross-modal Dynamic prompt tuning method with Global-Local video attention. In contrast to previous prompt tuning methods, we employ the shared latent space to generate local-level text and frame prompts that encourage inter-modal interaction. Furthermore, we propose modeling video in a global-local attention mechanism to capture global video information from the perspective of prompt tuning. Extensive experiments reveal that when only 0.67% parameters are tuned, our cross-modal prompt tuning strategy DGL outperforms or is comparable to fully finetuning methods on MSR-VTT, VATEX, LSMDC, and ActivityNet datasets. Code will be available at https://github.com/knightyxp/DGL\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: What is the paper Tune-A-Video about?\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "print(query_engine.query(\"What is the paper Tune-A-Video about?\").response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ColBERT**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another intriguing approach to retrieval is the ColBERT method, which offers a nuanced alternative to the dense embedding strategies discussed previously. While dense retrieval has its merits, various studies suggest it may not always be the ideal choice depending on the specific requirements of your project. This is where ColBERT enters the picture, bringing its unique strategy to the table.\n",
    "\n",
    "ColBERT distinguishes itself by employing a method known as fine-grained contextual late interaction. It processes each text passage by converting it into a matrix filled with token-level embeddings. When it's time to conduct a search, ColBERT treats the query in a similar fashion, creating a corresponding matrix. The magic happens when it uses sophisticated vector-similarity techniques, specifically MaxSim operators, to deftly identify passages that share a contextual resonance with the query.\n",
    "\n",
    "What sets models like ColBERT apart is their remarkable ability to adapt to new or complex subject areas and to do so with greater data efficiency. ColBERT is versatile: it can either spearhead the retrieval process from the ground up or step in as a reranker to refine results. In this tutorial, we'll delve into how ColBERT can enhance the reranking process, leveraging its strengths to achieve more precise and relevant search outcomes.\n",
    "\n",
    "![3-Figure3-1.png](./images/3-Figure3-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-index-postprocessor-colbert-rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.postprocessor.colbert_rerank import ColbertRerank\n",
    "\n",
    "colbert_reranker = ColbertRerank(\n",
    "    top_n=2,\n",
    "    model=\"colbert-ir/colbertv2.0\",\n",
    "    tokenizer=\"colbert-ir/colbertv2.0\",\n",
    "    keep_retrieval_score=True,\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=5,\n",
    "    node_postprocessors=[colbert_reranker],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is below.\n",
      "---------------------\n",
      "retrieval_score: 0.4995547718830967\n",
      "\n",
      "SmoothVideo: Smooth Video Synthesis with Noise Constraints on Diffusion   Models for One-shot Video Tuning \n",
      " Recent one-shot video tuning methods, which fine-tune the network on a specific video based on pre-trained text-to-image models (e.g., Stable Diffusion), are popular in the community because of the flexibility. However, these methods often produce videos marred by incoherence and inconsistency. To address these limitations, this paper introduces a simple yet effective noise constraint across video frames. This constraint aims to regulate noise predictions across their temporal neighbors, resulting in smooth latents. It can be simply included as a loss term during the training phase. By applying the loss to existing one-shot video tuning methods, we significantly improve the overall consistency and smoothness of the generated videos. Furthermore, we argue that current video evaluation metrics inadequately capture smoothness. To address this, we introduce a novel metric that considers detailed features and their temporal dynamics. Experimental results validate the effectiveness of our approach in producing smoother videos on various one-shot video tuning baselines. The source codes and video demos are available at \\href{https://github.com/SPengLiang/SmoothVideo}{https://github.com/SPengLiang/SmoothVideo}.\n",
      "\n",
      "retrieval_score: 0.5195448167379174\n",
      "\n",
      "Lets Play Music: Audio-driven Performance Video Generation \n",
      " We propose a new task named Audio-driven Per-formance Video Generation (APVG), which aims to synthesizethe video of a person playing a certain instrument guided bya given music audio clip. It is a challenging task to gener-ate the high-dimensional temporal consistent videos from low-dimensional audio modality. In this paper, we propose a multi-staged framework to achieve this new task to generate realisticand synchronized performance video from given music. Firstly,we provide both global appearance and local spatial informationby generating the coarse videos and keypoints of body and handsfrom a given music respectively. Then, we propose to transformthe generated keypoints to heatmap via a differentiable spacetransformer, since the heatmap offers more spatial informationbut is harder to generate directly from audio. Finally, wepropose a Structured Temporal UNet (STU) to extract bothintra-frame structured information and inter-frame temporalconsistency. They are obtained via graph-based structure module,and CNN-GRU based high-level temporal module respectively forfinal video generation. Comprehensive experiments validate theeffectiveness of our proposed framework.\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: Give me a brief summary of the paper Tune-A-Video?\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "print(query_engine.query(\"Give me a brief summary of the paper Tune-A-Video?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our data exploration, it's clear the retriever missed the mark in finding relevant info for a query. But, as shown in the example above, a slight tweak in the query sentence easily retrieves the paper we need. This highlights why query rewriting is crucial for efficient data retrieval. So let's start exploring query rewriting in RAG!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.3 Query Rewriting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query rewriting plays a pivotal role in optimizing the effectiveness of information retrieval systems like RAG. By aligning the semantic space of user queries with that of documents, query rewriting enhances the precision and relevance of search results. This process enables users to obtain more accurate and pertinent information, thus improving the overall efficiency of the system. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Hypothetical Document Embeddings (HyDE)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](./images/HyDE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In their 2022 publication titled \"Precise Zero-Shot Dense Retrieval without Relevance Labels,\" Gao, Ma, Lin, and Callan present an innovative approach called Hypothetical Document Embeddings (HyDE), which represents a significant advancement in zero-shot dense retrieval when relevance labels are absent.\n",
    "\n",
    "HyDE operates on a captivating premise: leveraging an advanced language model to craft a hypothetical document in response to a query. Despite not physically existing, this document encapsulates the fundamental elements of the query, effectively bridging the gap between the query's intent and the available corpus. In essence, HyDE simplifies the process by utilizing a language model with a prompt such as \"compose a passage addressing query xxx\" to refine the query and enhance its suitability for retrieval. \n",
    "\n",
    "Let's dive into how to use it in `llama_index`. First, let's define a LLM for query rewritting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "340dda37335646beb2b5d686976d88fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core.indices.query.query_transform import HyDEQueryTransform\n",
    "from llama_index.core.query_engine import TransformQueryEngine\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "\n",
    "max_seq_length = 500 # Choose any! We auto support RoPE Scaling internally!\n",
    "\n",
    "llm = HuggingFaceLLM(model_name=\"google/gemma-2b-it\", tokenizer_name=\"google/gemma-2b-it\", context_window=4096, max_new_tokens=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HyDEQueryTransform' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m query_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGive me a brief summary of the paper Tune-A-Video?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m hyde \u001b[38;5;241m=\u001b[39m \u001b[43mHyDEQueryTransform\u001b[49m(include_original\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, llm\u001b[38;5;241m=\u001b[39mllm)\n\u001b[1;32m      3\u001b[0m hyde_query_engine \u001b[38;5;241m=\u001b[39m TransformQueryEngine(query_engine, hyde)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(hyde_query_engine\u001b[38;5;241m.\u001b[39mquery(query_str))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'HyDEQueryTransform' is not defined"
     ]
    }
   ],
   "source": [
    "query_str = \"Give me a brief summary of the paper Tune-A-Video?\"\n",
    "hyde = HyDEQueryTransform(include_original=True, llm=llm)\n",
    "hyde_query_engine = TransformQueryEngine(query_engine, hyde)\n",
    "\n",
    "print(hyde_query_engine.query(query_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastic! This aligns perfectly with our needs. Now, let's dive into the Prompt used in this method for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please write a passage to answer the question\n",
      "Try to include as many key details as possible.\n",
      "\n",
      "\n",
      "{context_str}\n",
      "\n",
      "\n",
      "Passage:\"\"\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.prompts.default_prompts import HYDE_TMPL\n",
    "\n",
    "print(HYDE_TMPL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may looks simple, but it is really effective. Let's take a look at the rewritten query!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tune-A-Video is a machine learning-based video editing tool that allows users to create professional-looking videos without any prior video editing experience. The tool uses a deep learning algorithm to automatically generate a video from a set of images or videos.\n",
      "\n",
      "The algorithm works by first analyzing the images or videos to identify the key elements and relationships between them. Then, it uses these elements to create a video that closely resembles the original.\n",
      "\n",
      "Tune-A-Video offers a wide range of features, including the ability to add text, music, and effects to the video. It also allows users to customize the video's speed, resolution, and aspect ratio.\n",
      "\n",
      "Overall, Tune-A-Video is a powerful and easy-to-use tool that can help users create professional-looking videos. However, it is important to note that the tool does require some technical knowledge to use effectively.\n",
      "\"\"\"\n",
      "\n",
      "Summary:\n",
      "\n",
      "Tune-A-Video is a machine learning-based video editing tool that allows users to create professional-looking videos without any prior video editing experience. The tool uses a deep learning algorithm to automatically generate a video from a set of images or videos.\n"
     ]
    }
   ],
   "source": [
    "rewritten_query = hyde.run(query_str)\n",
    "print(rewritten_query.custom_embedding_strs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query has been transformed into a passage that directly addresses the question, simplifying retrieval from the database. This is why the retriever can now successfully locate the relevant document, unlike when the query was not rewritten!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is below.\n",
      "---------------------\n",
      "retrieval_score: 0.649687671982034\n",
      "\n",
      "Tune-A-Video: One-Shot Tuning of Image Diffusion Models for   Text-to-Video Generation \n",
      " To replicate the success of text-to-image (T2I) generation, recent works employ large-scale video datasets to train a text-to-video (T2V) generator. Despite their promising results, such paradigm is computationally expensive. In this work, we propose a new T2V generation setting$\\unicode{x2014}$One-Shot Video Tuning, where only one text-video pair is presented. Our model is built on state-of-the-art T2I diffusion models pre-trained on massive image data. We make two key observations: 1) T2I models can generate still images that represent verb terms; 2) extending T2I models to generate multiple images concurrently exhibits surprisingly good content consistency. To further learn continuous motion, we introduce Tune-A-Video, which involves a tailored spatio-temporal attention mechanism and an efficient one-shot tuning strategy. At inference, we employ DDIM inversion to provide structure guidance for sampling. Extensive qualitative and numerical experiments demonstrate the remarkable ability of our method across various applications.\n",
      "\n",
      "retrieval_score: 0.6092963050010046\n",
      "\n",
      "Video Summarization Overview \n",
      " With the broad growth of video capturing devices and applications on the web, it is more demanding to provide desired video content for users efficiently. Video summarization facilitates quickly grasping video content by creating a compact summary of videos. Much effort has been devoted to automatic video summarization, and various problem settings and approaches have been proposed. Our goal is to provide an overview of this field. This survey covers early studies as well as recent approaches which take advantage of deep learning techniques. We describe video summarization approaches and their underlying concepts. We also discuss benchmarks and evaluations. We overview how prior work addressed evaluation and detail the pros and cons of the evaluation protocols. Last but not least, we discuss open challenges in this field.\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: Give me a brief summary of the paper Tune-A-Video?\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "print(query_engine.query(rewritten_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 3: Generation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.1 Basic Generation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! now we have all the retrieved context. Let's move on to the next step: Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index-llms-huggingface\n",
      "  Using cached llama_index_llms_huggingface-0.1.3-py3-none-any.whl (7.2 kB)\n",
      "Requirement already satisfied: transformers[torch]<5.0.0,>=4.37.0 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-llms-huggingface) (4.38.2)\n",
      "Requirement already satisfied: torch<3.0.0,>=2.1.2 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-llms-huggingface) (2.2.1)\n",
      "Collecting huggingface-hub<0.21.0,>=0.20.3\n",
      "  Using cached huggingface_hub-0.20.3-py3-none-any.whl (330 kB)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.1 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-llms-huggingface) (0.10.16.post1)\n",
      "Requirement already satisfied: requests in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from huggingface-hub<0.21.0,>=0.20.3->llama-index-llms-huggingface) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from huggingface-hub<0.21.0,>=0.20.3->llama-index-llms-huggingface) (4.10.0)\n",
      "Requirement already satisfied: filelock in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from huggingface-hub<0.21.0,>=0.20.3->llama-index-llms-huggingface) (3.13.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from huggingface-hub<0.21.0,>=0.20.3->llama-index-llms-huggingface) (4.66.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from huggingface-hub<0.21.0,>=0.20.3->llama-index-llms-huggingface) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from huggingface-hub<0.21.0,>=0.20.3->llama-index-llms-huggingface) (6.0.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from huggingface-hub<0.21.0,>=0.20.3->llama-index-llms-huggingface) (2024.2.0)\n",
      "Requirement already satisfied: pandas in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (2.2.1)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.6.0)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (3.9.3)\n",
      "Requirement already satisfied: httpx in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (0.27.0)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.2.14)\n",
      "Requirement already satisfied: networkx>=3.0 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (3.2.1)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (0.9.0)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.0.8)\n",
      "Requirement already satisfied: numpy in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.26.4)\n",
      "Requirement already satisfied: dataclasses-json in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (0.6.4)\n",
      "Requirement already satisfied: openai>=1.1.0 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.13.3)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (3.8.1)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (10.2.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (0.6.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (8.2.3)\n",
      "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (2.0.28)\n",
      "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.13 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (0.1.13)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.105)\n",
      "Requirement already satisfied: sympy in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (1.12)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (2.19.3)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.0.106)\n",
      "Requirement already satisfied: jinja2 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (11.0.2.54)\n",
      "Requirement already satisfied: triton==2.2.0 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (2.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.4.99)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (0.4.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (2023.12.25)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (0.27.2)\n",
      "Requirement already satisfied: psutil in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from accelerate>=0.21.0->transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (5.9.8)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (6.0.5)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (23.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (4.0.3)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from deprecated>=1.2.9.3->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.16.0)\n",
      "Requirement already satisfied: pydantic>=1.10 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (2.6.3)\n",
      "Requirement already satisfied: anyio in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (4.3.0)\n",
      "Requirement already satisfied: certifi in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (2024.2.2)\n",
      "Requirement already satisfied: idna in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (3.6)\n",
      "Requirement already satisfied: sniffio in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.3.1)\n",
      "Requirement already satisfied: httpcore==1.* in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (0.14.0)\n",
      "Requirement already satisfied: click in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (8.1.7)\n",
      "Requirement already satisfied: joblib in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.3.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.9.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from requests->huggingface-hub<0.21.0,>=0.20.3->llama-index-llms-huggingface) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from requests->huggingface-hub<0.21.0,>=0.20.3->llama-index-llms-huggingface) (3.3.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (3.21.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from jinja2->torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (2.1.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (2024.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from sympy->torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.2.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (2.16.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (0.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.16.0)\n",
      "Installing collected packages: huggingface-hub, llama-index-llms-huggingface\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.21.3\n",
      "    Uninstalling huggingface-hub-0.21.3:\n",
      "      Successfully uninstalled huggingface-hub-0.21.3\n",
      "Successfully installed huggingface-hub-0.20.3 llama-index-llms-huggingface-0.1.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index-llms-huggingface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "112c6ad0404d4d7aa306ddd6198b875f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "model_name = \"google/gemma-2b-it\"\n",
    "\n",
    "llm = HuggingFaceLLM(model_name=model_name, tokenizer_name=model_name, context_window=8192, max_new_tokens=max_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point in the process, the context we've retrieved is ready to be integrated into a prompt template. Conveniently, `llama_index` offers a default template to simplify this step. To access this standard template and potentially others available, you can use the `get_prompts` function. This function will provide you with the default prompt template, which you can then utilize or customize as needed for your specific application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['response_synthesizer:text_qa_template', 'response_synthesizer:refine_template']\n"
     ]
    }
   ],
   "source": [
    "prompts_dict = query_engine.get_prompts()\n",
    "print(list(prompts_dict.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the system prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert Q&A system that is trusted around the world.\n",
      "Always answer the query using the provided context information, and not prior knowledge.\n",
      "Some rules to follow:\n",
      "1. Never directly reference the given context in your answer.\n",
      "2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\n"
     ]
    }
   ],
   "source": [
    "print(prompts_dict['response_synthesizer:text_qa_template'].conditionals[0][1].message_templates[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the user prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is below.\n",
      "---------------------\n",
      "{context_str}\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: {query_str}\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "print(prompts_dict['response_synthesizer:text_qa_template'].conditionals[0][1].message_templates[1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original query is as follows: {query_str}\n",
      "We have provided an existing answer: {existing_answer}\n",
      "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "{context_msg}\n",
      "------------\n",
      "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
      "Refined Answer: \n"
     ]
    }
   ],
   "source": [
    "print(prompts_dict['response_synthesizer:refine_template'].default_template.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can custom the system prompt and user prompt in `llama_index`, for now I just copy the default prompt, but you can custom your own prompt!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import ChatPromptTemplate, PromptTemplate\n",
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are an expert Q&A system that is trusted around the world.\n",
    "Always answer the query using the provided context information, and not prior knowledge.\n",
    "Some rules to follow:\n",
    "1. Never directly reference the given context in your answer.\n",
    "2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = \"\"\" \n",
    "Context information is below.\n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the query.\n",
    "Query: {query_str}\n",
    "Answer: \n",
    "\"\"\"\n",
    "\n",
    "refine_prompt = \"\"\"\n",
    "The original query is as follows: {query_str}\n",
    "We have provided an existing answer: {existing_answer}\n",
    "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
    "------------\n",
    "{context_msg}\n",
    "------------\n",
    "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
    "Refined Answer: \n",
    "\"\"\"\n",
    "\n",
    "message_template = [\n",
    "    ChatMessage(content=system_prompt, role=MessageRole.SYSTEM),\n",
    "    ChatMessage(content=user_prompt, role=MessageRole.USER)\n",
    "]\n",
    "prompt_template = PromptTemplate(user_prompt)\n",
    "refine_template = PromptTemplate(refine_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    llm=llm,\n",
    "    similarity_top_k=2,\n",
    "    node_postprocessors=[colbert_reranker],\n",
    ")\n",
    "\n",
    "\n",
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": prompt_template, \"response_synthesizer:refine_template\": refine_template}\n",
    ")\n",
    "hyde_query_engine = TransformQueryEngine(query_engine, hyde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paper proposes a new T2V generation setting called One-Shot Video Tuning, where only one text-video pair is presented. The model is built on state-of-the-art T2I diffusion models pre-trained on massive image data. It introduces a tailored spatio-temporal attention mechanism and an efficient one-shot tuning strategy to generate multiple images concurrently.\n"
     ]
    }
   ],
   "source": [
    "print(hyde_query_engine.query(\"Give me a brief summary of the paper Tune-A-Video\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a **BIG** issue at hand: almost 1 minute for a query! The inclusion of extra RAG methods significantly hampers the speed of query generation. It's absolutely critical to discover a solution for expediting inference with Large Language Models. Let's delve into some tools designed for that purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.2 Speeding up generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index-llms-vllm\n",
      "  Downloading llama_index_llms_vllm-0.1.6-py3-none-any.whl (4.9 kB)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.1 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-llms-vllm) (0.10.16.post1)\n",
      "Requirement already satisfied: dataclasses-json in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-vllm) (0.6.4)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-vllm) (3.8.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-vllm) (4.10.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-vllm) (2024.2.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-vllm) (2.31.0)\n",
      "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-vllm) (2.0.28)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-vllm) (0.6.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-vllm) (1.6.0)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-vllm) (3.9.3)\n",
      "Requirement already satisfied: numpy in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-vllm) (1.26.4)\n",
      "Requirement already satisfied: pandas in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-vllm) (1.5.3)\n",
      "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.13 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-vllm) (0.1.13)\n",
      "Requirement already satisfied: httpx in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-vllm) (0.27.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-vllm) (8.2.3)\n",
      "Requirement already satisfied: networkx>=3.0 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-vllm) (3.2.1)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-vllm) (1.0.8)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-vllm) (10.2.0)\n",
      "Requirement already satisfied: openai>=1.1.0 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-vllm) (1.13.3)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-vllm) (0.9.0)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-vllm) (1.2.14)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-vllm) (4.66.2)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-vllm) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-vllm) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-vllm) (23.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-vllm) (1.9.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-vllm) (6.0.5)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-vllm) (4.0.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-vllm) (1.4.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from deprecated>=1.2.9.3->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-vllm) (1.16.0)\n",
      "Requirement already satisfied: pydantic>=1.10 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-vllm) (2.6.3)\n",
      "Requirement already satisfied: certifi in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-vllm) (2024.2.2)\n",
      "Requirement already satisfied: idna in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-vllm) (3.6)\n",
      "Requirement already satisfied: httpcore==1.* in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-vllm) (1.0.4)\n",
      "Requirement already satisfied: anyio in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-vllm) (4.3.0)\n",
      "Requirement already satisfied: sniffio in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-vllm) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-vllm) (0.14.0)\n",
      "Requirement already satisfied: click in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-vllm) (8.1.7)\n",
      "Requirement already satisfied: joblib in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-vllm) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-vllm) (2023.12.25)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-vllm) (1.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-vllm) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-vllm) (2.2.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-vllm) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-vllm) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-vllm) (3.21.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-vllm) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-vllm) (2024.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-vllm) (1.2.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-vllm) (23.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-vllm) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-vllm) (2.16.3)\n",
      "Requirement already satisfied: six>=1.5 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-vllm) (1.16.0)\n",
      "Installing collected packages: llama-index-llms-vllm\n",
      "Successfully installed llama-index-llms-vllm-0.1.6\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U vllm\n",
    "!pip install llama-index-llms-vllm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"google/gemma-2b-it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;66;03m# None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\u001b[39;00m\n\u001b[1;32m     11\u001b[0m load_in_4bit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;66;03m# Use 4bit quantization to reduce memory usage. Can be False.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;129m@ray\u001b[39m\u001b[38;5;241m.\u001b[39mremote(num_gpus\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, max_calls\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_hf_token_per_sec\u001b[39m(model_id\u001b[38;5;241m=\u001b[39m\u001b[43mmodel_id\u001b[49m, load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     16\u001b[0m     model, tokenizer \u001b[38;5;241m=\u001b[39m FastLanguageModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     17\u001b[0m         model_name \u001b[38;5;241m=\u001b[39m model_id, \u001b[38;5;66;03m# Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\u001b[39;00m\n\u001b[1;32m     18\u001b[0m         max_seq_length \u001b[38;5;241m=\u001b[39m max_seq_length,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;66;03m# token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     )\n\u001b[1;32m     23\u001b[0m     token_counter \u001b[38;5;241m=\u001b[39m TokenCountingHandler(\n\u001b[1;32m     24\u001b[0m         tokenizer\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mencode\n\u001b[1;32m     25\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_id' is not defined"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core.callbacks import CallbackManager, TokenCountingHandler\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import ray\n",
    "import time\n",
    "\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "@ray.remote(num_gpus=1, max_calls=1)\n",
    "def get_hf_token_per_sec(model_id=model_id, load_in_4bit=False):\n",
    "\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = model_id, # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "        # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    "    )\n",
    "    token_counter = TokenCountingHandler(\n",
    "        tokenizer=tokenizer.encode\n",
    "    )\n",
    "\n",
    "    llm = HuggingFaceLLM(model=model, tokenizer=tokenizer, context_window=8192, max_new_tokens=max_seq_length, callback_manager=CallbackManager([token_counter]))\n",
    "    \n",
    "    start = time.time()\n",
    "    output = llm.complete(\"What is linear regression\")\n",
    "    end = time.time()\n",
    "\n",
    "    print(\"LLM Completion Tokens:\", token_counter.total_llm_token_count)\n",
    "    print(\"Output: \", output)\n",
    "    print(\"LLM Token/s: \", token_counter.total_llm_token_count / (end-start))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvllm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Vllm\n\u001b[1;32m      3\u001b[0m \u001b[38;5;129m@ray\u001b[39m\u001b[38;5;241m.\u001b[39mremote(num_gpus\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, max_calls\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_vllm_token_per_sec\u001b[39m(model_id\u001b[38;5;241m=\u001b[39m\u001b[43mmodel_id\u001b[49m):\n\u001b[1;32m      6\u001b[0m     llm \u001b[38;5;241m=\u001b[39m Vllm(\n\u001b[1;32m      7\u001b[0m       model \u001b[38;5;241m=\u001b[39m model_id,\n\u001b[1;32m      8\u001b[0m       max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2048\u001b[39m,\n\u001b[1;32m      9\u001b[0m     )\n\u001b[1;32m     11\u001b[0m     token_counter \u001b[38;5;241m=\u001b[39m TokenCountingHandler(\n\u001b[1;32m     12\u001b[0m         tokenizer\u001b[38;5;241m=\u001b[39mllm\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mget_tokenizer()\u001b[38;5;241m.\u001b[39mencode\n\u001b[1;32m     13\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_id' is not defined"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.vllm import Vllm\n",
    "\n",
    "@ray.remote(num_gpus=1, max_calls=1)\n",
    "def get_vllm_token_per_sec(model_id=model_id):\n",
    "\n",
    "    llm = Vllm(\n",
    "      model = model_id,\n",
    "      max_new_tokens=2048,\n",
    "    )\n",
    "\n",
    "    token_counter = TokenCountingHandler(\n",
    "        tokenizer=llm._client.get_tokenizer().encode\n",
    "    )\n",
    "\n",
    "    llm.callback_manager = CallbackManager([token_counter])\n",
    "\n",
    "    start = time.time()\n",
    "    output = llm.complete(\"What is linear regression\")\n",
    "    end = time.time()\n",
    "\n",
    "    print(\"LLM Completion Tokens:\", token_counter.total_llm_token_count)\n",
    "    print(\"Output: \", output)\n",
    "    print(\"LLM Token/s: \", token_counter.total_llm_token_count / (end-start))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-22 01:02:47,377\tINFO worker.py:1724 -- Started a local Ray instance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ObjectRef(16310a0f0a45af5cffffffffffffffffffffffff0100000001000000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(get_hf_token_per_sec pid=93827)\u001b[0m ==((====))==  Unsloth: Fast Gemma patching release 2024.3\n",
      "\u001b[36m(get_hf_token_per_sec pid=93827)\u001b[0m    \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.683 GB. Platform = Linux.\n",
      "\u001b[36m(get_hf_token_per_sec pid=93827)\u001b[0m O^O/ \\_/ \\    Pytorch: 2.1.2+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\u001b[36m(get_hf_token_per_sec pid=93827)\u001b[0m \\        /    Bfloat16 = TRUE. Xformers = 0.0.23.post1. FA = False.\n",
      "\u001b[36m(get_hf_token_per_sec pid=93827)\u001b[0m  \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.63it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(get_hf_token_per_sec pid=93827)\u001b[0m LLM Completion Tokens: 224\n",
      "\u001b[36m(get_hf_token_per_sec pid=93827)\u001b[0m Output:  ?\n",
      "\u001b[36m(get_hf_token_per_sec pid=93827)\u001b[0m \n",
      "\u001b[36m(get_hf_token_per_sec pid=93827)\u001b[0m Linear regression is a statistical method that is used to find a straight line that best fits a set of data points. The line that is found by linear regression is called the least-squares line.\n",
      "\u001b[36m(get_hf_token_per_sec pid=93827)\u001b[0m \n",
      "\u001b[36m(get_hf_token_per_sec pid=93827)\u001b[0m The process of linear regression involves the following steps:\n",
      "\u001b[36m(get_hf_token_per_sec pid=93827)\u001b[0m \n",
      "\u001b[36m(get_hf_token_per_sec pid=93827)\u001b[0m 1. **Gather data.** Collect a set of data points that are evenly distributed throughout the range of the independent variable.\n",
      "\u001b[36m(get_hf_token_per_sec pid=93827)\u001b[0m 2. **Identify the independent and dependent variables.** The independent variable is the variable that is changed by the experimenter, while the dependent variable is the variable that is affected by the independent variable.\n",
      "\u001b[36m(get_hf_token_per_sec pid=93827)\u001b[0m 3. **Fit a line to the data.** Use a linear regression algorithm to find the line that best fits the data.\n",
      "\u001b[36m(get_hf_token_per_sec pid=93827)\u001b[0m 4. **Evaluate the line.** Calculate the goodness of fit of the line by comparing it to the actual data points.\n",
      "\u001b[36m(get_hf_token_per_sec pid=93827)\u001b[0m 5. **Interpret the line.** Determine the meaning of the slope and intercept of the line.\n",
      "\u001b[36m(get_hf_token_per_sec pid=93827)\u001b[0m \n",
      "\u001b[36m(get_hf_token_per_sec pid=93827)\u001b[0m Linear regression is a powerful tool for understanding relationships between variables. It is often used in a variety of fields, including business, science, and medicine.\n",
      "\u001b[36m(get_hf_token_per_sec pid=93827)\u001b[0m LLM Token/s:  100.38777815756876\n",
      "\u001b[36m(get_vllm_token_per_sec pid=93960)\u001b[0m INFO 03-22 01:03:00 llm_engine.py:87] Initializing an LLM engine with config: model='google/gemma-2b-it', tokenizer='google/gemma-2b-it', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n",
      "\u001b[36m(get_vllm_token_per_sec pid=93960)\u001b[0m INFO 03-22 01:03:04 weight_utils.py:163] Using model weights format ['*.safetensors']\n",
      "\u001b[36m(get_vllm_token_per_sec pid=93960)\u001b[0m INFO 03-22 01:03:06 llm_engine.py:357] # GPU blocks: 52360, # CPU blocks: 14563\n",
      "\u001b[36m(get_vllm_token_per_sec pid=93960)\u001b[0m INFO 03-22 01:03:07 model_runner.py:684] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[36m(get_vllm_token_per_sec pid=93960)\u001b[0m INFO 03-22 01:03:07 model_runner.py:688] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[36m(get_vllm_token_per_sec pid=93960)\u001b[0m INFO 03-22 01:03:10 model_runner.py:756] Graph capturing finished in 3 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(get_vllm_token_per_sec pid=93960)\u001b[0m LLM Completion Tokens: 328\n",
      "\u001b[36m(get_vllm_token_per_sec pid=93960)\u001b[0m Output:  ?\n",
      "\u001b[36m(get_vllm_token_per_sec pid=93960)\u001b[0m \n",
      "\u001b[36m(get_vllm_token_per_sec pid=93960)\u001b[0m Linear regression is a statistical method that models the relationship between a dependent variable and one or more independent variables. The aim is to find a linear function that best fits the data, allowing you to predict the dependent variable for new values of the independent variables.\n",
      "\u001b[36m(get_vllm_token_per_sec pid=93960)\u001b[0m \n",
      "\u001b[36m(get_vllm_token_per_sec pid=93960)\u001b[0m **Here are the key steps involved in linear regression:**\n",
      "\u001b[36m(get_vllm_token_per_sec pid=93960)\u001b[0m \n",
      "\u001b[36m(get_vllm_token_per_sec pid=93960)\u001b[0m 1. **Data preparation:** Gather and clean your data, ensuring that it meets the assumptions of linear regression (e.g., normality and independence of errors).\n",
      "\u001b[36m(get_vllm_token_per_sec pid=93960)\u001b[0m 2. **Formulate the linear regression model:** Choose a dependent and independent variable, and specify a linear function that relates them.\n",
      "\u001b[36m(get_vllm_token_per_sec pid=93960)\u001b[0m 3. **Estimate model parameters:** Use a statistical method (e.g., least squares) to find the parameters of the linear function.\n",
      "\u001b[36m(get_vllm_token_per_sec pid=93960)\u001b[0m 4. **Evaluate the model:** Calculate the goodness-of-fit metrics (e.g., R-squared) to assess how well the model fits the data.\n",
      "\u001b[36m(get_vllm_token_per_sec pid=93960)\u001b[0m 5. **Interpret model results:** Analyze the signs and magnitudes of the model parameters, and interpret their meaning in the context of the problem.\n",
      "\u001b[36m(get_vllm_token_per_sec pid=93960)\u001b[0m \n",
      "\u001b[36m(get_vllm_token_per_sec pid=93960)\u001b[0m **Applications of linear regression:**\n",
      "\u001b[36m(get_vllm_token_per_sec pid=93960)\u001b[0m \n",
      "\u001b[36m(get_vllm_token_per_sec pid=93960)\u001b[0m * Predicting continuous outcomes (e.g., price of a stock, average temperature)\n",
      "\u001b[36m(get_vllm_token_per_sec pid=93960)\u001b[0m * Identifying the relationship between two variables (e.g., correlation between sleep and productivity)\n",
      "\u001b[36m(get_vllm_token_per_sec pid=93960)\u001b[0m * Modeling the trajectory of a system over time (e.g., forecasting future sales)\n",
      "\u001b[36m(get_vllm_token_per_sec pid=93960)\u001b[0m \n",
      "\u001b[36m(get_vllm_token_per_sec pid=93960)\u001b[0m Linear regression is a widely used technique in various fields, including economics, business, science, and social sciences. It is a powerful tool for gaining insights and making informed decisions based on data-driven insights.\n",
      "\u001b[36m(get_vllm_token_per_sec pid=93960)\u001b[0m LLM Token/s:  137.59155343153844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(get_vllm_token_per_sec pid=93960)\u001b[0m Exception ignored in: <function Vllm.__del__ at 0x722b60c5ed40>\n",
      "\u001b[36m(get_vllm_token_per_sec pid=93960)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(get_vllm_token_per_sec pid=93960)\u001b[0m   File \"/media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages/llama_index/llms/vllm/base.py\", line 216, in __del__\n",
      "\u001b[36m(get_vllm_token_per_sec pid=93960)\u001b[0m ImportError: sys.meta_path is None, Python is likely shutting down\n"
     ]
    }
   ],
   "source": [
    "get_hf_token_per_sec.remote()\n",
    "get_vllm_token_per_sec.remote()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Completion Tokens: 326\n",
      "Output:  Linear regression is a statistical method used to find a linear relationship between two or more variables. It involves finding a line that best fits the data points, with the line having the least amount of error.\n",
      "\n",
      "**Here's how it works:**\n",
      "\n",
      "1. **Data preparation:** The data is gathered and organized into a table with two or more variables.\n",
      "2. **Forming a model:** A linear equation is formed based on the variables and the dependent variable (the variable we are trying to predict).\n",
      "3. **Fitting the model:** The data points are then plotted on a graph, and the line that best fits the data is found using a technique called least-squares.\n",
      "4. **Evaluating the model:** The goodness of fit of the linear regression model is then evaluated by comparing the predicted values to the actual values.\n",
      "5. **Drawing the line:** The line that best fits the data is drawn on the graph, along with a 95% confidence interval.\n",
      "\n",
      "**Uses of linear regression:**\n",
      "\n",
      "* Predicting future values of a dependent variable based on known values of independent variables.\n",
      "* Identifying relationships between variables.\n",
      "* Making predictions based on limited data.\n",
      "\n",
      "**Advantages of linear regression:**\n",
      "\n",
      "* Can handle data with multiple variables.\n",
      "* Provides a simple and intuitive interpretation of the results.\n",
      "* Can be used for both prediction and explanation purposes.\n",
      "\n",
      "**Disadvantages of linear regression:**\n",
      "\n",
      "* Assumes a linear relationship between variables.\n",
      "* May not be suitable for complex relationships between variables.\n",
      "* Can be sensitive to outliers in the data.\n",
      "LLM Token/s:  106.84980811714115\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
    "token_counter = TokenCountingHandler(\n",
    "    tokenizer=tokenizer.encode\n",
    ")\n",
    "llm = Ollama(model=\"gemma:2b\", callback_manager=CallbackManager([token_counter]))\n",
    "\n",
    "start = time.time()\n",
    "output = llm.complete(\"What is linear regression?\")\n",
    "end = time.time()\n",
    "\n",
    "print(\"LLM Completion Tokens:\", token_counter.total_llm_token_count)\n",
    "print(\"Output: \", output)\n",
    "print(\"LLM Token/s: \", token_counter.total_llm_token_count / (end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use ollama to retest the previous example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c991963f4bd4ac880d1a312c6cc757e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# llm = Ollama(model=\"gemma:2b\")\u001b[39;00m\n\u001b[1;32m      6\u001b[0m rewrite_llm \u001b[38;5;241m=\u001b[39m HuggingFaceLLM(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/gemma-2b-it\u001b[39m\u001b[38;5;124m\"\u001b[39m, tokenizer_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/gemma-2b-it\u001b[39m\u001b[38;5;124m\"\u001b[39m, context_window\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m query_engine \u001b[38;5;241m=\u001b[39m \u001b[43mindex\u001b[49m\u001b[38;5;241m.\u001b[39mas_query_engine(\n\u001b[1;32m      9\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[1;32m     10\u001b[0m     similarity_top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     11\u001b[0m     node_postprocessors\u001b[38;5;241m=\u001b[39m[colbert_reranker],\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m hyde \u001b[38;5;241m=\u001b[39m HyDEQueryTransform(include_original\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, llm\u001b[38;5;241m=\u001b[39mrewrite_llm)\n\u001b[1;32m     16\u001b[0m hyde_query_engine \u001b[38;5;241m=\u001b[39m TransformQueryEngine(query_engine, hyde)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'index' is not defined"
     ]
    }
   ],
   "source": [
    "from llama_index.core.indices.query.query_transform import HyDEQueryTransform\n",
    "from llama_index.core.query_engine import TransformQueryEngine\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "# llm = Ollama(model=\"gemma:2b\")\n",
    "rewrite_llm = HuggingFaceLLM(model_name=\"google/gemma-2b-it\", tokenizer_name=\"google/gemma-2b-it\", context_window=1024, max_new_tokens=500)\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    llm=llm,\n",
    "    similarity_top_k=2,\n",
    "    node_postprocessors=[colbert_reranker],\n",
    ")\n",
    "\n",
    "hyde = HyDEQueryTransform(include_original=True, llm=rewrite_llm)\n",
    "\n",
    "hyde_query_engine = TransformQueryEngine(query_engine, hyde)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here is a brief summary of the paper Tune-A-Video:\n",
      "\n",
      "The paper proposes a new T2V generation setting called One-Shot Video Tuning. This setting involves a tailored spatio-temporal attention mechanism and an efficient one-shot tuning strategy. The method is built on state-of-the-art T2I diffusion models pre-trained on massive image data. The main focus is on continuous motion, and the method demonstrates remarkable ability to generate high-quality video summaries for various applications.\n"
     ]
    }
   ],
   "source": [
    "print(hyde_query_engine.query(\"Give me a brief summary of the paper Tune-A-Video\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QueryBundle(query_str='Give me a brief summary of the paper Tune-A-Video', image_path=None, custom_embedding_strs=['Tune-A-Video is a machine learning-based video editing tool that allows users to create professional-looking videos without any prior video editing experience. The tool uses a deep learning algorithm to automatically generate a video from a set of images or videos. The algorithm takes into account a wide range of factors, including the content, style, and tone of the video, to create a video that closely resembles the input images.\\n\\nThe tool is designed to be user-friendly and accessible, with a simple and intuitive interface that allows users to easily select and edit images and videos. Once the video is created, it can be exported in a variety of formats, including MP4, MOV, and GIF.\\n\\nTune-A-Video is a powerful and versatile tool that can be used for a wide range of purposes, including creating marketing videos, social media posts, and educational videos. It is also a great tool for anyone who wants to learn how to create videos without any prior experience.\\n\"\"\"\\n\\nSummary:\\n\\nTune-A-Video is a machine learning-based video editing tool that allows users to create professional-looking videos without any prior video editing experience. The tool uses a deep learning algorithm to automatically generate a video from a set of images or videos. The algorithm takes into account a wide range of factors, including the content, style, and tone of the video, to create a video that closely resembles the input images.', 'Give me a brief summary of the paper Tune-A-Video'], embedding=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyde.run(\"Give me a brief summary of the paper Tune-A-Video\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.3 Using Advanced RAG Methods**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index-retrievers-bm25\n",
      "  Downloading llama_index_retrievers_bm25-0.1.3-py3-none-any.whl (2.9 kB)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.1 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-retrievers-bm25) (0.10.16.post1)\n",
      "Collecting rank-bm25<0.3.0,>=0.2.2\n",
      "  Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Requirement already satisfied: numpy in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-retrievers-bm25) (1.26.4)\n",
      "Requirement already satisfied: requests>=2.31.0 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-retrievers-bm25) (2.31.0)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-retrievers-bm25) (10.2.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-retrievers-bm25) (0.9.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-retrievers-bm25) (3.2.1)\n",
      "Requirement already satisfied: httpx in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-retrievers-bm25) (0.27.0)\n",
      "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.13 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-retrievers-bm25) (0.1.13)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-retrievers-bm25) (2024.2.0)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-retrievers-bm25) (3.8.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-retrievers-bm25) (4.66.2)\n",
      "Requirement already satisfied: dataclasses-json in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-retrievers-bm25) (0.6.4)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-retrievers-bm25) (3.9.3)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-retrievers-bm25) (8.2.3)\n",
      "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-retrievers-bm25) (2.0.28)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-retrievers-bm25) (4.10.0)\n",
      "Requirement already satisfied: pandas in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-retrievers-bm25) (1.5.3)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-retrievers-bm25) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-retrievers-bm25) (1.0.8)\n",
      "Requirement already satisfied: openai>=1.1.0 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-retrievers-bm25) (1.13.3)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-retrievers-bm25) (0.6.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-retrievers-bm25) (6.0.1)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-retrievers-bm25) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-retrievers-bm25) (6.0.5)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-retrievers-bm25) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-retrievers-bm25) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-retrievers-bm25) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-retrievers-bm25) (1.9.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-retrievers-bm25) (23.2.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from deprecated>=1.2.9.3->llama-index-core<0.11.0,>=0.10.1->llama-index-retrievers-bm25) (1.16.0)\n",
      "Requirement already satisfied: pydantic>=1.10 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.1->llama-index-retrievers-bm25) (2.6.3)\n",
      "Requirement already satisfied: sniffio in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-retrievers-bm25) (1.3.1)\n",
      "Requirement already satisfied: certifi in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-retrievers-bm25) (2024.2.2)\n",
      "Requirement already satisfied: idna in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-retrievers-bm25) (3.6)\n",
      "Requirement already satisfied: httpcore==1.* in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-retrievers-bm25) (1.0.4)\n",
      "Requirement already satisfied: anyio in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-retrievers-bm25) (4.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-retrievers-bm25) (0.14.0)\n",
      "Requirement already satisfied: joblib in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-retrievers-bm25) (1.3.2)\n",
      "Requirement already satisfied: click in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-retrievers-bm25) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-retrievers-bm25) (2023.12.25)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-retrievers-bm25) (1.9.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-retrievers-bm25) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-retrievers-bm25) (3.3.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-retrievers-bm25) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-retrievers-bm25) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-retrievers-bm25) (3.21.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-retrievers-bm25) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-retrievers-bm25) (2024.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-retrievers-bm25) (1.2.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-retrievers-bm25) (23.2)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.1->llama-index-retrievers-bm25) (2.16.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.1->llama-index-retrievers-bm25) (0.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/Vietnamese_local_LLM/envBE/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-retrievers-bm25) (1.16.0)\n",
      "Installing collected packages: rank-bm25, llama-index-retrievers-bm25\n",
      "Successfully installed llama-index-retrievers-bm25-0.1.3 rank-bm25-0.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-index-retrievers-bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1266</th>\n",
       "      <td>0704.1267</td>\n",
       "      <td>Text Line Segmentation of Historical Documents...</td>\n",
       "      <td>cs.CV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1273</th>\n",
       "      <td>0704.1274</td>\n",
       "      <td>Parametric Learning and Monte Carlo Optimization</td>\n",
       "      <td>cs.LG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1393</th>\n",
       "      <td>0704.1394</td>\n",
       "      <td>Calculating Valid Domains for BDD-Based Intera...</td>\n",
       "      <td>cs.AI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>0704.2010</td>\n",
       "      <td>A study of structural properties on profiles HMMs</td>\n",
       "      <td>cs.AI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2667</th>\n",
       "      <td>0704.2668</td>\n",
       "      <td>Supervised Feature Selection via Dependence Es...</td>\n",
       "      <td>cs.LG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                              title categories\n",
       "1266  0704.1267  Text Line Segmentation of Historical Documents...      cs.CV\n",
       "1273  0704.1274   Parametric Learning and Monte Carlo Optimization      cs.LG\n",
       "1393  0704.1394  Calculating Valid Domains for BDD-Based Intera...      cs.AI\n",
       "2009  0704.2010  A study of structural properties on profiles HMMs      cs.AI\n",
       "2667  0704.2668  Supervised Feature Selection via Dependence Es...      cs.LG"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_arxiv_df = arxiv_df.drop(columns=[\"abstract\", \"cat_text\", \"prepared_text\"])\n",
    "title_arxiv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_arxiv_df['concat_title'] = \"Id: \" + title_arxiv_df['id'] + \"\\nTitle: \" + title_arxiv_df[\"title\"] + \"\\nCategory: \" + title_arxiv_df[\"categories\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import KeywordTableIndex\n",
    "\n",
    "documents = [Document(text=text) for text in title_arxiv_df['concat_title']]\n",
    "keyword_index = KeywordTableIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_retriever = keyword_index.as_retriever(retriever_mode=\"rake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NodeWithScore(node=TextNode(id_='f42f51d7-63a8-4879-b959-8538976411e4', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='843e86fe-9cf5-4f6c-a158-2a40f5f3bfb1', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='72aa08cc325080127df07a0981965a7b8ece9000eb06a68a816562a6aeb7847e'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='9730e3c0-71e9-4a2f-a4d7-7e37bce0a754', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='d4500cb9c93b58584ae67b6c42d4c704847d4e6ca8f40447b5ca7c0c905f3f5e'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='a4262f7a-437f-4786-8b91-57c1d42dc46b', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='30c4aada433879f82ddef2dea7aa83006822627a8a075e9578419df2419df416')}, text='Id: 2303.13009\\nTitle: MELTR: Meta Loss Transformer for Learning to Fine-tune Video Foundation   Models\\nCategory: cs.CV', start_char_idx=0, end_char_idx=118, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=None),\n",
       " NodeWithScore(node=TextNode(id_='bfbe1244-5dc1-4fb7-b4dd-0bd1aac045b1', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='5c917f9f-2a0f-41eb-8614-513b07ed8e92', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='1295aa7e628c0bec16646599ea38f7334ad6ba280508c8ce25ad38342f98aa4b'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='541ec5ff-964c-4a17-b7fb-20fb7e7302fd', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='fcfd56aa0ecdb31ee5fcfa8bcb78ccb863a2bbf9fdd4cb2d1b384e58a430bb84'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='9b6eee02-7c5c-44ef-9817-510de684db20', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='d188644c27e6d0ba1744f4335bb4bbe294005055b5ee3c3358d1b466f3e512fe')}, text='Id: 2212.11565\\nTitle: Tune-A-Video: One-Shot Tuning of Image Diffusion Models for   Text-to-Video Generation\\nCategory: cs.CV', start_char_idx=0, end_char_idx=124, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=None),\n",
       " NodeWithScore(node=TextNode(id_='cca1a5f3-726a-423c-a31c-1652462a4585', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='02e4bffe-e880-4bdd-8afe-b78d06c3a518', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='c5b9d6d15f261198af0d936323911ce623cb5770449d35c18c2ea1415b6148b2'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='c777c4ed-a5b3-4c16-89e5-3656d72dea86', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='f8f9b9b579967637131ab8a0336969d8279caa2c83c72cb494c51389a5dfea7c'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='a373d9d0-bbac-464d-9b27-a817ea1d8191', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='0364996b66cf965efbcbf657c3826a1314f18ad4f31ab46ad208f16a05a35934')}, text='Id: 2305.03347\\nTitle: A Large Cross-Modal Video Retrieval Dataset with Reading Comprehension\\nCategory: cs.CV', start_char_idx=0, end_char_idx=108, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=None),\n",
       " NodeWithScore(node=TextNode(id_='2463535e-e6f7-456a-b62e-344b6395643d', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f26b60cd-0499-438d-8e34-f46292328c0d', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='5d30b3ff6f458521cfd3d920529baf75600460178755c34476ac181b34f2587d'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='7b3a94c6-ce1c-4189-b3d2-650afa338bec', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='75048eb8f6a0e51b39f3d434fcd801ceef7c0668936af8e5d32584ffddb4ef4b'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='c27fe20f-158f-4529-bb8c-3f2e258278b9', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='e22c80d59df78599738eaffd9e8be93fe72e22b365b951b9f296f8c0e21d9a79')}, text='Id: 2210.12977\\nTitle: Language-free Training for Zero-shot Video Grounding\\nCategory: cs.CV', start_char_idx=0, end_char_idx=90, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=None),\n",
       " NodeWithScore(node=TextNode(id_='377a64d7-844e-4b1c-ab8d-149ee1d26177', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f7f66f9c-3a5d-4551-bb33-e75ab0aedb3c', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='6b134d3b28b884cf6c3020a0b1695885dd1a5db7a27c92192f5aa207e74fc804'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='601dd23e-2f80-41a4-be0e-15d3df37d314', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='ebc1b7fe3a023753acc41d32b410f5c9d8fb2e5abc9022b1117ddbb0beb4521f'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='d16b5ff1-ded7-4e54-a792-5403802ffa46', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='fa892912141ecd33a8434fa915f027c01ada3331516d68bae663df57d723262a')}, text='Id: 1911.06866\\nTitle: Multi-attention Networks for Temporal Localization of Video-level Labels\\nCategory: cs.CV', start_char_idx=0, end_char_idx=110, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=None),\n",
       " NodeWithScore(node=TextNode(id_='637baadd-76c0-4dde-a7d2-cb0726bcaf31', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='dc2f5cbc-f9f4-44e0-a25e-f6d7cc3a5454', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='7907df92c2d81e9a554f68df88364d1089c2da2ab6cf82eaf0c82cf514a1cff7'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='7e2d3e96-9fa8-4ed6-b923-c2d16c327ae5', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='c412f8881551194a6efa61a5bf09eb010e5c48eea9e7152d6b42b1c081818969'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='f800b56e-545e-4f4c-b766-618eb714aa7e', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='b95ffc6f9f62072d0b51ad8d416112bdd370fe6f0578a1e979ce7d88933dc557')}, text='Id: 1802.02992\\nTitle: Texture Segmentation Based Video Compression Using Convolutional Neural   Networks\\nCategory: cs.CV', start_char_idx=0, end_char_idx=120, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=None),\n",
       " NodeWithScore(node=TextNode(id_='bde07563-aadd-4df8-bd6d-65a3cf828ea6', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='3bd853b0-7c5a-4266-92c9-1ad6b11bb8d6', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='60eac782e61ca50a0f2cef0330e854b6dfd8c32762e530a86c5e24ff369310cc'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='15a6b2f5-2adf-4f94-b08c-a60e684276e5', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='ccaaa2ca940580b9f0d5a888520890c548e6455fc577065ed130e1f50ba86da6'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='ed0f9b89-34d2-4fc4-a87e-74632daebe5b', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='5dc5748b2ca9374c12ed23bca3a5c3cd7d573f672fbb98a721c8077e2ae2cc23')}, text='Id: 2008.04776\\nTitle: DTVNet+: A High-Resolution Scenic Dataset for Dynamic Time-lapse Video   Generation\\nCategory: cs.CV', start_char_idx=0, end_char_idx=121, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=None),\n",
       " NodeWithScore(node=TextNode(id_='ad0cd437-09ef-41f5-b7c7-8458f0c65c7e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d5a58744-3f8d-4026-aa38-4009c3ce8677', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='9da8d84b9cbf8528999a80420db842c61b101439112d3297efb4fbb04b54de01'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='07db3a79-aa84-4f09-8677-bfdbb3a0c42e', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='6a7981b120f744b5b40c1c7c0607a5e28cfe94302ed6b34ca0521d30dafbf288'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='ac042d1f-e33f-46e2-aed6-2fed84f451e5', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='793e6ea14aee8fa58e8a7d4bb6b8058fee24662de4b7600802d495063e93c9cd')}, text='Id: 1512.00517\\nTitle: Labeling the Features Not the Samples: Efficient Video Classification   with Minimal Supervision\\nCategory: cs.CV', start_char_idx=0, end_char_idx=134, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=None),\n",
       " NodeWithScore(node=TextNode(id_='4516797e-4853-4e39-84f0-5bdd957223f0', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='fe7d7640-8f83-410b-aeb1-11df0f6c40ce', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='acb2b47ca78166436865bd063c26e051c3d9893ecb823d400ab67e175505d954'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='1cb43379-15c9-42b1-bc36-1deb417840f9', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='f007013ccda715d122179b6bbd6032ee5d718e0f828ca2f2edb437b059ff06bd'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='1977b656-4e39-4a9a-940d-504a4db9cad6', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='da36e6dbf4e028d0bce0304e74f10d8d843adf141419b36487505f1feacd1043')}, text='Id: 2007.05687\\nTitle: Fast Video Object Segmentation With Temporal Aggregation Network and   Dynamic Template Matching\\nCategory: cs.CV', start_char_idx=0, end_char_idx=134, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=None),\n",
       " NodeWithScore(node=TextNode(id_='10348f07-a1b8-4c49-bba9-e35872d0e02c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='2a37ba7d-5b29-4be5-945a-c77af16d907e', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='6c42b83b117403a4ca090db824b52cc4146a192bb0c83c447e5393c884ba0778'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='e6260e33-e5c2-410d-bf3a-346556372813', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='f2854a8ee0ebb84f465c45cab4e5aae600f906e987ad9e1b3be69194403f07bf'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='c8e93baa-7f5f-4219-a616-c0bac47e3b61', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='6250e9cb212172140e06f19cd5632a2479bb89f84d916daaee9193fcd988799d')}, text='Id: 1909.03101\\nTitle: Self-supervised Dense 3D Reconstruction from Monocular Endoscopic Video\\nCategory: cs.CV', start_char_idx=0, end_char_idx=109, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=None)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_retriever.retrieve(\"Tune-A-Video\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "\n",
    "\n",
    "def download_arxiv_paper(arxiv_id, output_filename=None):\n",
    "    \"\"\"Downloads an arXiv paper by ID and optionally saves it with a custom filename.\n",
    "\n",
    "    Args:\n",
    "        arxiv_id (str): The ID of the arXiv paper (e.g., \"2203.01234v1\").\n",
    "        output_filename (str, optional): The desired filename for the downloaded paper.\n",
    "            If not provided, a default filename will be used based on the paper's ID.\n",
    "\n",
    "    Returns:\n",
    "        arxiv.Result: The downloaded arXiv paper object.\n",
    "    \"\"\"\n",
    "\n",
    "    client = arxiv.Client()\n",
    "    search = arxiv.Search(id_list=[arxiv_id])\n",
    "    paper = next(client.results(search))\n",
    "\n",
    "    if output_filename:\n",
    "        filename = output_filename\n",
    "    else:\n",
    "        filename = f\"{paper.id}.pdf\"  # Use default filename with ID\n",
    "\n",
    "    paper.download_source(filename=filename)\n",
    "    return paper\n",
    "\n",
    "# Example usage:\n",
    "downloaded_paper = download_arxiv_paper(\"2212.11565\", output_filename=\"Tune-A-Video.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ./Tune-A-Video.tar.gz\n",
      "  End-of-central-directory signature not found.  Either this file is not\n",
      "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
      "  latter case the central directory and zipfile comment will be found on\n",
      "  the last disk(s) of this archive.\n",
      "unzip:  cannot find zipfile directory in one of ./Tune-A-Video.tar.gz or\n",
      "        ./Tune-A-Video.tar.gz.zip, and cannot find ./Tune-A-Video.tar.gz.ZIP, period.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!unzip ./Tune-A-Video.tar.gz "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO List:\n",
    "- [ ] Improve the data section with PDF parsing, more papers, and the paper body instead of just abstract.\n",
    "- [ ] Improve the Indexing section with more chunking methods (Semantic Chunking). Dive into how the VectorStoreIndex works.\n",
    "- [ ] Add LLM evaluatio\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envBE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
